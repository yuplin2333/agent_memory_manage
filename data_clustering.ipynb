{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clustering\n",
    "\n",
    "This is how we obtain the `distribshift` data split (see `./data_patch/split.json`) used in our *distribution shift* experiments.\n",
    "\n",
    "The following code is just for demonstration. Some modification is needed to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from typing import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector_keys(data_dict):\n",
    "    vx = data_dict[\"ego_states\"][0] * 0.5\n",
    "    vy = data_dict[\"ego_states\"][1] * 0.5\n",
    "    v_yaw = data_dict[\"ego_states\"][4]\n",
    "    ax = (\n",
    "        data_dict[\"ego_hist_traj_diff\"][-1, 0]\n",
    "        - data_dict[\"ego_hist_traj_diff\"][-2, 0]\n",
    "    )\n",
    "    ay = (\n",
    "        data_dict[\"ego_hist_traj_diff\"][-1, 1]\n",
    "        - data_dict[\"ego_hist_traj_diff\"][-2, 1]\n",
    "    )\n",
    "    cx = data_dict[\"ego_states\"][2]\n",
    "    cy = data_dict[\"ego_states\"][3]\n",
    "    vhead = data_dict[\"ego_states\"][7] * 0.5\n",
    "    steeling = data_dict[\"ego_states\"][8]\n",
    "\n",
    "    return [\n",
    "        np.array([vx, vy, v_yaw, ax, ay, cx, cy, vhead, steeling]),\n",
    "        data_dict[\"goal\"],\n",
    "        data_dict[\"ego_hist_traj\"].flatten(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "with open(\"./data/split.json\", \"r\") as f:\n",
    "    split_data = json.load(f)\n",
    "val_tokens = split_data[\"val\"]\n",
    "token_list = random.sample(val_tokens, 2000)\n",
    "assert token_list[0] == \"6a1f2ebe1ef8437c87a5a742362b09b4\"\n",
    "token_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gt traj\n",
    "with open(\"./data/metrics/gt_traj.pkl\", \"rb\") as f:\n",
    "    all_gt_traj = pickle.load(f)\n",
    "\n",
    "gt_traj = {key: all_gt_traj[key] for key in token_list}\n",
    "gt_traj[\"6a1f2ebe1ef8437c87a5a742362b09b4\"] # shape: (1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualize test_set_memory\n",
    "test_mem_input = []  # will be keys\n",
    "test_mem_output = []  # will be trajectories\n",
    "test_mem_joint = []  # will be keys + trajectories\n",
    "\n",
    "for token in token_list:\n",
    "    file_path = f\"./data/val/{token}.pkl\"\n",
    "    sample = pickle.load(open(file_path, \"rb\"))\n",
    "    data_dict = {\n",
    "        \"ego_states\": sample[\"ego_states\"],\n",
    "        \"goal\": sample[\"goal\"],\n",
    "        \"ego_hist_traj\": sample[\"ego_hist_traj\"],\n",
    "        \"ego_hist_traj_diff\": sample[\"ego_hist_traj_diff\"],\n",
    "    }\n",
    "    key = gen_vector_keys(data_dict)\n",
    "    key = np.concatenate(key, axis=0)\n",
    "    test_mem_input.append(key)\n",
    "\n",
    "    traj = gt_traj[token][0].flatten()\n",
    "    test_mem_output.append(traj)\n",
    "\n",
    "    concat_input_output = np.concatenate([key, traj])\n",
    "    test_mem_joint.append(concat_input_output)\n",
    "\n",
    "\n",
    "print(len(test_mem_input), len(test_mem_output), len(test_mem_joint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_input = TSNE(n_components=2, random_state=0, perplexity=30, n_iter=300)\n",
    "test_mem_input_tsne = tsne_input.fit_transform(np.array(test_mem_input))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(test_mem_input_tsne[:, 0], test_mem_input_tsne[:, 1], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_output = TSNE(n_components=2, random_state=0, perplexity=30, n_iter=300)\n",
    "test_mem_output_tsne = tsne_output.fit_transform(np.array(test_mem_output))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(test_mem_output_tsne[:, 0], test_mem_output_tsne[:, 1], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_concat = TSNE(n_components=2, random_state=0, perplexity=30, n_iter=300)\n",
    "test_mem_joint_tsne = tsne_concat.fit_transform(np.array(test_mem_joint))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(test_mem_joint_tsne[:, 0], test_mem_joint_tsne[:, 1], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM cluster input\n",
    "# fit a new gmm\n",
    "gmm_input = GaussianMixture(n_components=3, random_state=0, covariance_type=\"diag\")\n",
    "gmm_input.fit(test_mem_input)\n",
    "test_mem_input_labels = gmm_input.predict(test_mem_input)\n",
    "\n",
    "# Plot input\n",
    "plt.figure()\n",
    "# scatter with labeling and legend\n",
    "for i in range(3):\n",
    "    plt.scatter(\n",
    "        test_mem_input_tsne[test_mem_input_labels == i, 0],\n",
    "        test_mem_input_tsne[test_mem_input_labels == i, 1],\n",
    "        marker=\".\",\n",
    "        label=f\"cluster {i}\",\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(Counter(test_mem_input_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a new list of tokens following the order\n",
    "new_token_list = []\n",
    "for label in [0, 1, 2]:\n",
    "    for i, l in enumerate(test_mem_input_labels):\n",
    "        if l == label:\n",
    "            new_token_list.append(token_list[i])\n",
    "\n",
    "print(len(new_token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/split_distribution_shift_everything.json\", \"w\") as f:\n",
    "    json.dump({\"distribshift2\": new_token_list}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split first 180 and rest to two different files\n",
    "token_list_part1 = new_token_list[:180]\n",
    "token_list_part2 = new_token_list[180:]\n",
    "\n",
    "with open(\"./data/split_part1.json\", \"w\") as f:\n",
    "    json.dump({\"cluster0_180mem\": token_list_part1}, f)\n",
    "\n",
    "with open(\"./data/split_part2.json\", \"w\") as f:\n",
    "    json.dump({\"cluster01_restmem\": token_list_part2}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new list of tokens\n",
    "with open(\"./data/split_val_distributionshift_joint_01.json\", \"w\") as f:\n",
    "    json.dump(new_token_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a reverse lookup with test_mem_input_labels\n",
    "reverse_lookup = {token: int(label) for token, label in zip(token_list, test_mem_joint_labels)}\n",
    "# save it\n",
    "with open(\"./data/reverse_lookup_val_distributionshift_joint_01.json\", \"w\") as f:\n",
    "    json.dump(reverse_lookup, f, indent=4)\n",
    "reverse_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster-wise Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reverse_lookup\n",
    "with open(\"./data/reverse_lookup_val_distributionshift_joint_01.json\", \"r\") as f:\n",
    "    reverse_lookup = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILE = \"./results/[EXPERIMENT_NAME]/prediction_results.pkl\"\n",
    "\n",
    "with open(RESULT_FILE, \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# results[\"64d019bf33ba4dcb9eca5e5ab2ef967e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the results into clusters, put into a dictionary\n",
    "cluster_results = {0: {}, 1: {}, 2: {}}\n",
    "for token, result in results.items():\n",
    "    label = reverse_lookup[token]\n",
    "    cluster_results[label][token] = result\n",
    "\n",
    "print(len(cluster_results[0]), len(cluster_results[1]), len(cluster_results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentdriver.evaluation.evaluation import planning_evaluation\n",
    "\n",
    "class AttributeDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "for label, cluster_result in cluster_results.items():\n",
    "    # eval_config = {\n",
    "    #     \"method\": f\"Distribution Cluster {label}\",\n",
    "    #     \"metric\": \"uniad\",\n",
    "    #     \"gt_folder\": \"data/metrics\",\n",
    "    #     # \"success_threshold\": 7.5,\n",
    "    # }\n",
    "    eval_config = AttributeDict(\n",
    "        {\n",
    "            \"method\": f\"Cluster {label}\",\n",
    "            \"metric\": \"uniad\",\n",
    "            \"gt_folder\": \"data/metrics\",\n",
    "            \"success_threshold\": 7.5,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    planning_evaluation(cluster_result, eval_config, success_threshold=7.5)\n",
    "    # planning_evaluation(cluster_result, eval_config, success_threshold=5.0)\n",
    "    # planning_evaluation(cluster_result, eval_config, success_threshold=2.5)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-driver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
