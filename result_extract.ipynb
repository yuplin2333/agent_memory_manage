{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results Extractor\n",
    "\n",
    "Use this notebook to extract results from `.pkl` to `.json`.\n",
    "\n",
    "JSON files will be saved in `./_json_results/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIFY THE BLOCK BELOW\n",
    "\n",
    "Below is an example of how you should provide the *input folder name* in `./results/` folder, and the *output dict key* in the merged JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_FOLDERS = [\n",
    "    # [\"RESULT_FOLDER_NAME\", \"DICT_KEY\"]\n",
    "    [\"example_addcoarse_exp\", \"addcoarse\"],\n",
    "    [\"example_addstrict_exp\", \"addstrict\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIFY THE BLOCK BELOW\n",
    "\n",
    "This will sort results in the exact token order provided in `./data/split.json`. To re-order the results tokens to the *distribution shift* experiment settings, comment out *NORMAL* section and un-comment *DISTRIBUTION SHIFT* section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "with open(\"./data/split.json\", \"r\") as f:\n",
    "    split_data = json.load(f)\n",
    "\n",
    "\n",
    "# Uncomment the corresponding section to obtain ordered results.\n",
    "\n",
    "#### NORMAL\n",
    "val_tokens = split_data[\"val\"]\n",
    "token_list = random.sample(val_tokens, 2000)\n",
    "assert token_list[0] == \"6a1f2ebe1ef8437c87a5a742362b09b4\"\n",
    "\n",
    "#### DISTRIBUTION SHIFT\n",
    "# val_tokens = split_data[\"distribshift\"]\n",
    "# token_list = val_tokens\n",
    "# assert token_list[0] == \"64d019bf33ba4dcb9eca5e5ab2ef967e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (FOLDER, DICT_TITLE) in RECORD_FOLDERS:\n",
    "    with open(f\"./results/{FOLDER}/records.pkl\", \"rb\") as f:\n",
    "        records = pickle.load(f)\n",
    "    df_records = pd.DataFrame(records)\n",
    "\n",
    "    skip_list = []\n",
    "    pointer = 0\n",
    "    accumulated_succ = 0\n",
    "    result = {\n",
    "        f\"{DICT_TITLE}\": {\n",
    "            \"input_similarity_top1_demo\": [],\n",
    "            \"output_similarity_cos\": [],\n",
    "            \"output_similarity_exp\": [],\n",
    "            \"acc_all_steps\": [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for i, token in enumerate(token_list):\n",
    "        # locate record[\"token\"]\n",
    "        record = df_records[df_records[\"token\"] == token]\n",
    "        # if not found, input sim = None, output sim = None, acc = -1\n",
    "        if len(record) == 0:\n",
    "            skip_list.append(i)\n",
    "            result[DICT_TITLE][\"input_similarity_top1_demo\"].append(None)\n",
    "            result[DICT_TITLE][\"output_similarity_cos\"].append(None)\n",
    "            result[DICT_TITLE][\"output_similarity_exp\"].append(None)\n",
    "            result[DICT_TITLE][\"acc_all_steps\"].append(-1)\n",
    "            continue\n",
    "        record = record.iloc[0]\n",
    "        input_sim = record[\"input_similarity\"]\n",
    "        output_cos_sim = record[\"output_similarity_cos\"]\n",
    "        output_exp_sim = record[\"output_similarity_exp\"]\n",
    "        is_success = record[\"l2_error\"] < 2.5\n",
    "\n",
    "        result[DICT_TITLE][\"input_similarity_top1_demo\"].append(input_sim)\n",
    "        result[DICT_TITLE][\"output_similarity_cos\"].append(output_cos_sim)\n",
    "        result[DICT_TITLE][\"output_similarity_exp\"].append(output_exp_sim)\n",
    "        result[DICT_TITLE][\"acc_all_steps\"].append(1 if is_success else 0)\n",
    "\n",
    "    assert len(skip_list) < 200 # sanity check. Should skip less than ~200 results. Too large means code wrong\n",
    "    print(f\"{DICT_TITLE} skips: {len(skip_list)}\")\n",
    "    assert len(result[DICT_TITLE][\"acc_all_steps\"]) == 2000, f\"{DICT_TITLE} has {len(result[DICT_TITLE]['acc_all_steps'])} records\"\n",
    "    # save dict\n",
    "    os.makedirs(\"./_json_results\", exist_ok=True)\n",
    "    save_path = f\"./_json_results/{DICT_TITLE}.json\"\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every json file in \"./_json_results\", load it and merge it\n",
    "merged_json = {}\n",
    "\n",
    "JSON_RESULTS_PATH = \"./_json_results\"\n",
    "for file in os.listdir(JSON_RESULTS_PATH):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(os.path.join(JSON_RESULTS_PATH, file), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_json.update(data)\n",
    "\n",
    "with open(\"./_json_results/_merged_json.json\", \"w\") as f:\n",
    "    json.dump(merged_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for json_file in merged_json:\n",
    "    print(f\"{json_file}: {len(merged_json[json_file]['acc_all_steps'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-driver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
